1,16d0
< /**
<  * Copyright 2019 Pinterest, Inc.
<  *
<  * Licensed under the Apache License, Version 2.0 (the "License");
<  * you may not use this file except in compliance with the License.
<  * You may obtain a copy of the License at
<  *
<  *    http://www.apache.org/licenses/LICENSE-2.0
<  *
<  * Unless required by applicable law or agreed to in writing, software
<  * distributed under the License is distributed on an "AS IS" BASIS,
<  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
<  * See the License for the specific language governing permissions and
<  * limitations under the License.
<  */
< 
20d3
< import com.pinterest.singer.loggingaudit.thrift.LoggingAuditHeaders;
21a5
> import com.pinterest.singer.loggingaudit.thrift.LoggingAuditHeaders;
32a17,18
> import org.apache.thrift.protocol.TBinaryProtocol;
> import org.apache.thrift.protocol.TProtocolFactory;
48c34,38
<  *  LoggingAuditEventSender implementations that dequeue LoggingAuditEvent and send it to Kafka.
---
>  * Copyright 2019 Pinterest, Inc.
>  *
>  * Licensed under the Apache License, Version 2.0 (the "License");
>  * you may not use this file except in compliance with the License.
>  * You may obtain a copy of the License at
50c40,46
<  *  Each instance of this class should be executed in its own thread.
---
>  *    http://www.apache.org/licenses/LICENSE-2.0
>  *
>  * Unless required by applicable law or agreed to in writing, software
>  * distributed under the License is distributed on an "AS IS" BASIS,
>  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
>  * See the License for the specific language governing permissions and
>  * limitations under the License.
51a48
> 
60,93d56
<   /**
<    *  When sender send audit events to Kafka,  it chooses a random partition and if it fails, it
<    *  will choose another GOOD partition, this retry will happen at most numOfPartitionsToTrySending
<    *  times before dropping the event. Note that, this is different from the retry handled by
<    *  kafka client
<    *  library when sending an event to a certain partition.
<    */
<   private static final int NUM_OF_PARTITIONS_TO_TRY_SENDING = 3;
< 
<   /**
<    *  maximum time to wait when sender tries to dequeue events before returning null.
<    */
<   private static final int DEQUEUE_WAIT_IN_SECONDS = 30;
< 
< 
<   /**
<    *   when gracefully shutting down the Sender, the calling thread sleeps for some time and let
<    *   the sender to send out audit events left in the queue if there are any.
<    *   stopGracePeriodInSeconds is the maximum time reserved and
<    *   stopGracePeriodInSeconds / THREAD_SLEEP_IN_SECONDS is the total rounds the calling thread
<    *   will sleep.
<    */
<   private static final int THREAD_SLEEP_IN_SECONDS = 10;
< 
<   /**
<    *  when gracefully shutting down the Sender, this field specifies maximum time for main thread
<    *  to wait, in order to let the sender send out audit events left in the queue if there are any.
<    */
<   private int stopGracePeriodInSeconds = 300;
< 
< 
<   /**
<    * Logging audit stage, can be THRIFTLOGGER, SINGER, MERCED and so on
<    */
96,98d58
<   /**
<    * host name
<    */
101,110d60
<   /**
<    *  LinkedBlockingDequeue to store the LoggingAuditEvents.  The max capacity is specified when
<    *  creating this deque in the LoggingAuditClient.java
<    *
<    *  The AuditEventKafkaSender dequeue from the beginning of the deque, if does not send out event
<    *  successfully, it will enqueue this event to the beginning of the queue.  Note that, if enqueue
<    *  the failed event at the end of the queue, this event could be processed with quite some delay
<    *  and this is not the behavior we want.  That's we use LinkedBlockingDequeue,
<    *  not ArrayBlockingQueue.
<    */
113,115d62
<   /**
<    * KafkaProducer instance to send audit events
<    */
118,121c65
<   /**
<    * Serialize key and value to byte[]
<    */
<   private TSerializer serializer = new TSerializer();
---
>   private TSerializer serializer;
123,125d66
<   /**
<    *  flag to control the start and stop of the executing thread.
<    */
128,130d68
<   /**
<    *  topic to store the audit events
<    */
133,135d70
<   /**
<    *  name of this sender instance
<    */
138,140d72
<   /**
<    *  executing thread
<    */
143,145d74
<   /**
<    *  List of PartitionInfo
<    */
148,150d76
<   /**
<    *  last time when partition list was refreshed. we want to refresh partition list every 5 mins.
<    */
153,156d78
<   /**
<    *  If sending out to one partition fails, this partition is added to set;
<    *  If sending out to one partition succeeds, this partition is removed if it was added before.
<    */
159,165d80
<   /**
<    * For each event (identified by LoggingAuditHeaders, key of the map), track the number of tries
<    * for sending to Kafka. Each try will choose a different and partition that is not in the
<    * badPartitions.  When event is send out successfully or dropped, the corresponding entry in
<    * this map is removed.
<    */
< 
168,175d82
<   /**
<    *  currentPartitionId specifies the partition of audit_event topic used to receive audit events.
<    *  The currentPartitionId will be reset in resetCurrentPartitionIdIfNeeded() method. This reduces
<    *  the number of TCP connections from audit client to the Kafka Cluster hosting the audit_event
<    *  topic.
<    */
<   private int currentPartitionId = -1;
< 
184a92
>     this.serializer = new TSerializer(new TBinaryProtocol.Factory());
188,196d95
< 
<   public KafkaProducer<byte[], byte[]> getKafkaProducer() {
<     return kafkaProducer;
<   }
< 
<   public void setKafkaProducer(KafkaProducer<byte[], byte[]> kafkaProducer) {
<     this.kafkaProducer = kafkaProducer;
<   }
< 
198d96
<     // refresh every 30 seconds
205,206c103,104
<         OpenTsdbMetricConverter.incr(
<             LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_PARTITIONS_REFRESH_COUNT, 1,
---
>         OpenTsdbMetricConverter
>             .incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_PARTITIONS_REFRESH_COUNT, 1,
227c125
<         int randomPartition = partitionInfoList.get(index).partition();
---
>         int randomPartition = partitionInfoList.get(index).partition());
231,233c129,130
<           currentPartitionId = randomPartition;
<           OpenTsdbMetricConverter.incr(
<               LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_CURRENT_PARTITION_RESET, 1,
---
>           OpenTsdbMetricConverter
>               .incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_KAFKA_CURRENT_PARTITION_RESET, 1,
234a132
>           currentPartitionId = randomPartition;
240,243c138
<       LOG.warn("After {} trials, set current partition to {}",
<           MAX_RETRIES_FOR_SELECTION_RANDOM_PARTITION, currentPartitionId);
< 
<     }
---
>       LOG.warn("After {} trials, set current partition to {}", MAX_RETRIES_FOR_SELECTION_RANDOM_PARTITION, currentPartitionId);
245,264d139
< 
<   /**
<    *  Sender dequeues LoggingAuditEvents and sends them to Kafka cluster. If send to one partition
<    *  fails, it will choose another partition. For each event, it will try at most
<    *  NUM_OF_PARTITIONS_TO_TRY_SENDING times (3 times) before dropping this event. Metrics are
<    *  used to track the queue size and usuage, number of events sent out to Kafka successfully, and
<    *  the number of events dropped.
<    */
<   @Override
<   public void run() {
<     LoggingAuditEvent event = null;
<     ProducerRecord<byte[], byte[]> record;
<     byte[] value = null;
< 
<     while (!cancelled.get()) {
<       try {
<         refreshPartitionIfNeeded();
<         if (currentPartitionId == -1){
<           Thread.sleep(100);
<           continue;
266,297d140
<         event = queue.poll(DEQUEUE_WAIT_IN_SECONDS, TimeUnit.SECONDS);
<         if (event != null) {
<           try {
<             value = serializer.serialize(event);
<             record = new ProducerRecord<>(this.topic, currentPartitionId , null, value);
<             kafkaProducer.send(record, new KafkaProducerCallback(event, currentPartitionId));
<           } catch (TException e) {
<             LOG.debug("[{}] failed to construct ProducerRecord because of serialization exception.",
<                 Thread.currentThread().getName(), e);
<             OpenTsdbMetricConverter
<                 .incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_SERIALIZATION_EXCEPTION, 1,
<                     "host=" + host, "stage=" + stage.toString(),
<                     "logName=" + event.getLoggingAuditHeaders().getLogName());
<             eventTriedCount.remove(event.getLoggingAuditHeaders());
<           }
<         }
<       } catch (InterruptedException e) {
<         LOG.warn("[{}] got interrupted when polling the queue and while loop is ended!",
<             Thread.currentThread().getName(), e);
<         OpenTsdbMetricConverter.incr(
<             LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_DEQUEUE_INTERRUPTED_EXCEPTION, 1,
<                 "host=" + host, "stage=" + stage.toString());
<         break;
<       } catch (Exception e) {
<         LOG.warn("Exit the while loop and finish the thread execution due to exception: ", e);
<         OpenTsdbMetricConverter.incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_EXCEPTION, 1,
<             "host=" + host, "stage=" + stage.toString());
<         break;
<       }
<     }
<   }
< 
310d152
<       // if exception thrown (i.e. the send failed), the partition is added to badPartitions.
314,319c156
<               "host=" + host, "stage=" + stage.toString(), "topic=" + topic,
<               "partition=" + this.partition);
< 
<       // retry the failed event by inserting it at the beginning of the deque.
<       // If number of tries reaches 3, meaning that 3 partitions have been tried sending to but
<       // still failed, this event is dropped.
---
>               "host=" + host, "stage=" + stage.toString(), "topic=" + topic);
350,353c187,188
<         LOG.debug(
<             "Enqueuing LoggingAuditEvent at head of the queue was interrupted in callback. "
<                 + "Drop this event");
<         eventTriedCount.remove(event.getLoggingAuditHeaders());
---
>         LOG.warn("[{}] got interrupted while waiting for [{}] to send out LoggingAuditEvents left"
>             + " in the queue.", Thread.currentThread().getName(), name, ex);
365,367d199
< 
<           // if send is successful, remove the event from the map eventTriedCount if it was added
<           // LoggingAuditHeaders can uniquely identify an event.
369d200
<           // if send out successfully, remove the partition from the badPartitions if it was added.
375c206
<         LOG.warn("Exception throws in the callback. Drop this event {}", event, t);
---
>         LOG.warn("Exception is thrown in the callback. Drop this event {}", event, t);
383,385c214,252
<   /**
<    *  start the executing thread and let the Sender run.
<    */
---
>   public void run() {
>     LoggingAuditEvent event = null;
>     ProducerRecord<byte[], byte[]> record;
>     byte[] value = null;
> 
>     while (!cancelled.get()) {
>       try {
>         refreshPartitionIfNeeded();
>         if (currentPartitionId == -1){
>           Thread.sleep(100);
>           continue;
>         }
>         event = queue.poll(DEQUEUE_WAIT_IN_SECONDS, TimeUnit.SECONDS);
>         if (event != null) {
>           try {
>             value = serializer.serialize(event);
>             record = new ProducerRecord<>(this.topic, currentPartitionId, null, value);
>             kafkaProducer.send(record, new KafkaProducerCallback(event, currentPartitionId));
>           } catch (TException e) {
>             LOG.debug("[{}] failed to construct ProducerRecord because of serialization exception.",
>                 Thread.currentThread().getName(), e);
>             OpenTsdbMetricConverter
>                 .incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_SERIALIZATION_EXCEPTION, 1,
>                     "host=" + host, "stage=" + stage.toString());
>             eventTriedCount.remove(event.getLoggingAuditHeaders());
>           }
>         }
>       } catch (InterruptedException e) {
>         LOG.warn("[{}] got interrupted while polling the queue and while loop is ended!",
>             Thread.currentThread().getName(), e);
>       } catch (Exception e) {
>         LOG.warn("Exit the while loop and finish the thread execution due to exception: ", e);
>         OpenTsdbMetricConverter
>             .incr(LoggingAuditClientMetrics.AUDIT_CLIENT_SENDER_EXCEPTION, 1,
>                 "host=" + host, "stage=" + stage.toString());
>       }
>     }
>   }
> 
398,401d264
<   /**
<    *  reserve some time (by default 30 seconds at most)to let AuditEventKafkaSender to send out
<    *  LoggingAuditEvent in the queue and gracefully stop AuditEventKafkaSender.
<    */
406c269
<         Thread.currentThread().getName(), stopGracePeriodInSeconds, name);
---
>         Thread.currentThread().name(), stopGracePeriodInSeconds, name);
415c278
<             Thread.currentThread().getName(), THREAD_SLEEP_IN_SECONDS, queue.size());
---
>             Thread.currentThread().name(), THREAD_SLEEP_IN_SECONDS, queue.size());
418c281
<             + "in the queue.", Thread.currentThread().getName(), name, e);
---
>             + " in the queue.", Thread.currentThread().name(), name, e);
433d295
< 
